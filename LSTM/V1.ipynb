{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/30 - Loss: 0.3921\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93      5885\n",
      "         1.0       0.00      0.00      0.00       901\n",
      "\n",
      "    accuracy                           0.87      6786\n",
      "   macro avg       0.43      0.50      0.46      6786\n",
      "weighted avg       0.75      0.87      0.81      6786\n",
      "\n",
      "Epoch 2/30 - Loss: 0.3891\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93      5885\n",
      "         1.0       0.00      0.00      0.00       901\n",
      "\n",
      "    accuracy                           0.87      6786\n",
      "   macro avg       0.43      0.50      0.46      6786\n",
      "weighted avg       0.75      0.87      0.81      6786\n",
      "\n",
      "Epoch 3/30 - Loss: 0.3889\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93      5885\n",
      "         1.0       0.00      0.00      0.00       901\n",
      "\n",
      "    accuracy                           0.87      6786\n",
      "   macro avg       0.43      0.50      0.46      6786\n",
      "weighted avg       0.75      0.87      0.81      6786\n",
      "\n",
      "Epoch 4/30 - Loss: 0.3889\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93      5885\n",
      "         1.0       0.00      0.00      0.00       901\n",
      "\n",
      "    accuracy                           0.87      6786\n",
      "   macro avg       0.43      0.50      0.46      6786\n",
      "weighted avg       0.75      0.87      0.81      6786\n",
      "\n",
      "Epoch 5/30 - Loss: 0.3889\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93      5885\n",
      "         1.0       0.00      0.00      0.00       901\n",
      "\n",
      "    accuracy                           0.87      6786\n",
      "   macro avg       0.43      0.50      0.46      6786\n",
      "weighted avg       0.75      0.87      0.81      6786\n",
      "\n",
      "Epoch 6/30 - Loss: 0.3887\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93      5885\n",
      "         1.0       0.00      0.00      0.00       901\n",
      "\n",
      "    accuracy                           0.87      6786\n",
      "   macro avg       0.43      0.50      0.46      6786\n",
      "weighted avg       0.75      0.87      0.81      6786\n",
      "\n",
      "Epoch 7/30 - Loss: 0.3888\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93      5885\n",
      "         1.0       0.00      0.00      0.00       901\n",
      "\n",
      "    accuracy                           0.87      6786\n",
      "   macro avg       0.43      0.50      0.46      6786\n",
      "weighted avg       0.75      0.87      0.81      6786\n",
      "\n",
      "Epoch 8/30 - Loss: 0.3802\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93      5885\n",
      "         1.0       0.00      0.00      0.00       901\n",
      "\n",
      "    accuracy                           0.87      6786\n",
      "   macro avg       0.43      0.50      0.46      6786\n",
      "weighted avg       0.75      0.87      0.81      6786\n",
      "\n",
      "Epoch 9/30 - Loss: 0.3546\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93      5885\n",
      "         1.0       0.33      0.00      0.00       901\n",
      "\n",
      "    accuracy                           0.87      6786\n",
      "   macro avg       0.60      0.50      0.47      6786\n",
      "weighted avg       0.80      0.87      0.81      6786\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# File paths\n",
    "data_dir = \"/home/yugdes/snapDetection/dataV0\"\n",
    "model_output_path = \"/home/yugdes/snapDetection/LSTM/models/attention_lstm.pt\"\n",
    "os.makedirs(os.path.dirname(model_output_path), exist_ok=True)\n",
    "\n",
    "train_file = os.path.join(data_dir, \"joint_velocity_train.csv\")\n",
    "val_file = os.path.join(data_dir, \"joint_velocity_val.csv\")\n",
    "test_file = os.path.join(data_dir, \"joint_velocity_test.csv\")\n",
    "\n",
    "# Hyperparameters\n",
    "WINDOW_SIZE = 30\n",
    "NUM_JOINTS = 7\n",
    "TAIL_SIZE = 5  # Use last 5 time stamps for labeling\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_SIZE = 64\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#######################################\n",
    "# Dataset Class with Sliding Windows #\n",
    "#######################################\n",
    "class SnapWindowedDataset(Dataset):\n",
    "    def __init__(self, csv_file, window_size=WINDOW_SIZE, num_joints=NUM_JOINTS, tail_size=TAIL_SIZE, scaler=None, fit_scaler=False):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # Assume columns with \"vel\" in their name are joint velocities and one column \"label\" exists.\n",
    "        joint_cols = [col for col in df.columns if \"vel\" in col]\n",
    "        label_col = \"label\"\n",
    "\n",
    "        data = df[joint_cols].values  # shape: (total_time_steps, num_joints)\n",
    "        labels = df[label_col].values  # shape: (total_time_steps,)\n",
    "\n",
    "        # Initialize or use provided scalers for each joint (robot-specific normalization)\n",
    "        self.scaler = scaler if scaler else [MinMaxScaler() for _ in range(num_joints)]\n",
    "        if fit_scaler:\n",
    "            for j in range(num_joints):\n",
    "                self.scaler[j].fit(data[:, j].reshape(-1, 1))\n",
    "        # Apply normalization per joint\n",
    "        for j in range(num_joints):\n",
    "            data[:, j] = self.scaler[j].transform(data[:, j].reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Create sliding windows:\n",
    "        self.X, self.y = [], []\n",
    "        for i in range(len(data) - window_size + 1):\n",
    "            window = data[i:i + window_size]  # shape: (window_size, num_joints)\n",
    "            # Transpose to get shape: (num_joints, window_size)\n",
    "            window = window.T  \n",
    "            # Labeling strategy: look at the last 'tail_size' time steps.\n",
    "            # If any of these time steps is positive, label the entire window as positive.\n",
    "            tail_labels = labels[i + window_size - tail_size: i + window_size]\n",
    "            label = 1 if np.max(tail_labels) == 1 else 0\n",
    "            self.X.append(window)\n",
    "            self.y.append(label)\n",
    "\n",
    "        self.X = torch.tensor(np.stack(self.X), dtype=torch.float32)\n",
    "        self.y = torch.tensor(self.y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "##########################################\n",
    "# Attention-based LSTM Model Definition  #\n",
    "##########################################\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, num_joints, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.num_joints = num_joints\n",
    "        # We'll process each joint's time series independently.\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        # Attention layer: one score per joint feature vector\n",
    "        self.attn_fc = nn.Linear(hidden_size, 1)\n",
    "        # Classifier to decide snap event based on aggregated features.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, num_joints, window_size)\n",
    "        joint_features = []\n",
    "        for j in range(self.num_joints):\n",
    "            # For each joint, extract its time series: (batch, window_size)\n",
    "            joint_seq = x[:, j, :].unsqueeze(-1)  # shape: (batch, window_size, 1)\n",
    "            out, _ = self.lstm(joint_seq)  \n",
    "            # Use the last output of the LSTM as the joint's feature representation\n",
    "            last_hidden = out[:, -1, :]  # shape: (batch, hidden_size)\n",
    "            joint_features.append(last_hidden)\n",
    "\n",
    "        # Stack features from all joints: (batch, num_joints, hidden_size)\n",
    "        joint_features = torch.stack(joint_features, dim=1)\n",
    "        # Compute attention scores and weights for each joint\n",
    "        attn_scores = self.attn_fc(joint_features).squeeze(-1)  # (batch, num_joints)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)          # (batch, num_joints)\n",
    "        # Aggregate features using the computed attention weights\n",
    "        attn_output = torch.sum(attn_weights.unsqueeze(-1) * joint_features, dim=1)  # (batch, hidden_size)\n",
    "        # Classify the aggregated feature\n",
    "        out = self.classifier(attn_output).squeeze()\n",
    "        return out\n",
    "\n",
    "#########################\n",
    "# Data Loading Section  #\n",
    "#########################\n",
    "train_set = SnapWindowedDataset(train_file, fit_scaler=True)\n",
    "val_set = SnapWindowedDataset(val_file, scaler=train_set.scaler)\n",
    "test_set = SnapWindowedDataset(test_file, scaler=train_set.scaler)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "#############################\n",
    "# Training and Evaluation   #\n",
    "#############################\n",
    "model = AttentionLSTM(NUM_JOINTS, input_size=1, hidden_size=HIDDEN_SIZE).to(DEVICE)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation after each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds, val_targets = [], []\n",
    "        for X, y in val_loader:\n",
    "            X = X.to(DEVICE)\n",
    "            preds = model(X).cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_targets.extend(y.numpy())\n",
    "\n",
    "        # Binary threshold at 0.5\n",
    "        val_preds_bin = [1 if p > 0.5 else 0 for p in val_preds]\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss/len(train_loader):.4f}\")\n",
    "        print(\"Validation Classification Report:\")\n",
    "        print(classification_report(val_targets, val_preds_bin, zero_division=0))\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), model_output_path)\n",
    "print(f\"Model saved to {model_output_path}\")\n",
    "\n",
    "# Final Evaluation on Test Data\n",
    "model.eval()\n",
    "test_preds, test_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for X, y in test_loader:\n",
    "        X = X.to(DEVICE)\n",
    "        preds = model(X).cpu().numpy()\n",
    "        test_preds.extend(preds)\n",
    "        test_targets.extend(y.numpy())\n",
    "\n",
    "test_preds_bin = [1 if p > 0.5 else 0 for p in test_preds]\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(test_targets, test_preds_bin, zero_division=0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yug_snapfit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
